{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 10 - Kubernetes Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, clone the repo and build the Docker image in WSL:\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/DataTalksClub/machine-learning-zoomcamp.git\n",
    "cd machine-learning-zoomcamp/cohorts/2025/05-deployment/homework\n",
    "docker build -f Dockerfile_full -t zoomcamp-model:3.13.10-hw10 .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Test Docker Image\n",
    "\n",
    "Run in WSL terminal 1:\n",
    "```bash\n",
    "docker run -it --rm -p 9696:9696 zoomcamp-model:3.13.10-hw10\n",
    "```\n",
    "\n",
    "Run in WSL terminal 2:\n",
    "```bash\n",
    "python q6_test.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q6_test.py content - create this file\n",
    "q6_test_code = '''import requests\n",
    "\n",
    "url = \"http://localhost:9696/predict\"\n",
    "\n",
    "client = {\n",
    "    \"job\": \"management\",\n",
    "    \"duration\": 400,\n",
    "    \"poutcome\": \"success\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=client).json()\n",
    "print(response)\n",
    "'''\n",
    "\n",
    "with open('q6_test.py', 'w') as f:\n",
    "    f.write(q6_test_code)\n",
    "print(\"q6_test.py created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Kind Version\n",
    "\n",
    "Run in WSL:\n",
    "```bash\n",
    "kind --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cluster\n",
    "\n",
    "```bash\n",
    "kind create cluster\n",
    "kubectl cluster-info\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Smallest Deployable Unit\n",
    "\n",
    "Answer: **Pod**\n",
    "\n",
    "A Pod is the smallest deployable computing unit in Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Service Type\n",
    "\n",
    "Run in WSL:\n",
    "```bash\n",
    "kubectl get services\n",
    "```\n",
    "\n",
    "The default `kubernetes` service has type **ClusterIP**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Load Docker Image to Kind\n",
    "\n",
    "Answer: **kind load docker-image**\n",
    "\n",
    "Run in WSL:\n",
    "```bash\n",
    "kind load docker-image zoomcamp-model:3.13.10-hw10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Deployment Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_yaml = '''apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: subscription\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: subscription\n",
    "  replicas: 1\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: subscription\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: subscription\n",
    "        image: zoomcamp-model:3.13.10-hw10\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"64Mi\"\n",
    "            cpu: \"100m\"            \n",
    "          limits:\n",
    "            memory: \"128Mi\"\n",
    "            cpu: \"500m\"\n",
    "        ports:\n",
    "        - containerPort: 9696\n",
    "'''\n",
    "\n",
    "with open('deployment.yaml', 'w') as f:\n",
    "    f.write(deployment_yaml)\n",
    "print(\"deployment.yaml created\")\n",
    "print(\"\\nPort value: 9696\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply deployment in WSL:\n",
    "```bash\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl get pods\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Service Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_yaml = '''apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: subscription\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  selector:\n",
    "    app: subscription\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 9696\n",
    "'''\n",
    "\n",
    "with open('service.yaml', 'w') as f:\n",
    "    f.write(service_yaml)\n",
    "print(\"service.yaml created\")\n",
    "print(\"\\nAnswer for <???>: subscription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply service in WSL:\n",
    "```bash\n",
    "kubectl apply -f service.yaml\n",
    "kubectl get services\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Service\n",
    "\n",
    "Port forward in WSL:\n",
    "```bash\n",
    "kubectl port-forward service/subscription 9696:80\n",
    "```\n",
    "\n",
    "Then run q6_test.py in another terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoscaling Setup\n",
    "\n",
    "```bash\n",
    "kubectl autoscale deployment subscription --name subscription-hpa --cpu-percent=20 --min=1 --max=3\n",
    "kubectl get hpa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8: Load Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_test_code = '''import requests\n",
    "from time import sleep\n",
    "\n",
    "url = \"http://localhost:9696/predict\"\n",
    "\n",
    "client = {\n",
    "    \"job\": \"management\",\n",
    "    \"duration\": 400,\n",
    "    \"poutcome\": \"success\"\n",
    "}\n",
    "\n",
    "while True:\n",
    "    sleep(0.1)\n",
    "    response = requests.post(url, json=client).json()\n",
    "    print(response)\n",
    "'''\n",
    "\n",
    "with open('load_test.py', 'w') as f:\n",
    "    f.write(load_test_code)\n",
    "print(\"load_test.py created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run load test:\n",
    "```bash\n",
    "# Terminal 1: Watch HPA\n",
    "kubectl get hpa subscription-hpa --watch\n",
    "\n",
    "# Terminal 2: Run load test\n",
    "python load_test.py\n",
    "```\n",
    "\n",
    "Max replicas should scale up to **3** under load."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
