{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df64264b-b1c4-4834-8962-d7ea07c2b2bd",
   "metadata": {},
   "source": [
    "## ML Zoomcamp 2025  - HW 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c9e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298ff4e3-76e8-416e-875d-a6035e1e56ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    lead_source       industry  number_of_courses_viewed  annual_income  \\\n",
      "0      paid_ads            NaN                         1        79450.0   \n",
      "1  social_media         retail                         1        46992.0   \n",
      "2        events     healthcare                         5        78796.0   \n",
      "3      paid_ads         retail                         2        83843.0   \n",
      "4      referral      education                         3        85012.0   \n",
      "5        events  manufacturing                         1        59904.0   \n",
      "6  social_media     technology                         0        51283.0   \n",
      "7  social_media            NaN                         5        62975.0   \n",
      "8      referral     healthcare                         4        38648.0   \n",
      "9      paid_ads          other                         3        59866.0   \n",
      "\n",
      "  employment_status       location  interaction_count  lead_score  converted  \n",
      "0        unemployed  south_america                  4        0.94          1  \n",
      "1          employed  south_america                  1        0.80          0  \n",
      "2        unemployed      australia                  3        0.69          1  \n",
      "3               NaN      australia                  1        0.87          0  \n",
      "4     self_employed         europe                  3        0.62          1  \n",
      "5               NaN         africa                  6        0.83          1  \n",
      "6               NaN    middle_east                  2        0.57          0  \n",
      "7           student         europe                  4        0.62          1  \n",
      "8        unemployed  south_america                  2        0.86          1  \n",
      "9           student      australia                  3        0.43          1  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('course_lead_scoring.csv')\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10e0c862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1462 entries, 0 to 1461\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   lead_source               1334 non-null   object \n",
      " 1   industry                  1328 non-null   object \n",
      " 2   number_of_courses_viewed  1462 non-null   int64  \n",
      " 3   annual_income             1281 non-null   float64\n",
      " 4   employment_status         1362 non-null   object \n",
      " 5   location                  1399 non-null   object \n",
      " 6   interaction_count         1462 non-null   int64  \n",
      " 7   lead_score                1462 non-null   float64\n",
      " 8   converted                 1462 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(4)\n",
      "memory usage: 102.9+ KB\n",
      "lead_source                 128\n",
      "industry                    134\n",
      "number_of_courses_viewed      0\n",
      "annual_income               181\n",
      "employment_status           100\n",
      "location                     63\n",
      "interaction_count             0\n",
      "lead_score                    0\n",
      "converted                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ac7ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684be623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "df[numerical_cols]= df[numerical_cols].fillna(0.0)\n",
    "df[categorical_cols] = df[categorical_cols].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c40d112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead_source                 0\n",
      "industry                    0\n",
      "number_of_courses_viewed    0\n",
      "annual_income               0\n",
      "employment_status           0\n",
      "location                    0\n",
      "interaction_count           0\n",
      "lead_score                  0\n",
      "converted                   0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13384add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    lead_source       industry  number_of_courses_viewed  annual_income  \\\n",
      "0      paid_ads        missing                         1        79450.0   \n",
      "1  social_media         retail                         1        46992.0   \n",
      "2        events     healthcare                         5        78796.0   \n",
      "3      paid_ads         retail                         2        83843.0   \n",
      "4      referral      education                         3        85012.0   \n",
      "5        events  manufacturing                         1        59904.0   \n",
      "6  social_media     technology                         0        51283.0   \n",
      "7  social_media        missing                         5        62975.0   \n",
      "8      referral     healthcare                         4        38648.0   \n",
      "9      paid_ads          other                         3        59866.0   \n",
      "\n",
      "  employment_status       location  interaction_count  lead_score  converted  \n",
      "0        unemployed  south_america                  4        0.94          1  \n",
      "1          employed  south_america                  1        0.80          0  \n",
      "2        unemployed      australia                  3        0.69          1  \n",
      "3           missing      australia                  1        0.87          0  \n",
      "4     self_employed         europe                  3        0.62          1  \n",
      "5           missing         africa                  6        0.83          1  \n",
      "6           missing    middle_east                  2        0.57          0  \n",
      "7           student         europe                  4        0.62          1  \n",
      "8        unemployed  south_america                  2        0.86          1  \n",
      "9           student      australia                  3        0.43          1  \n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c93a98f",
   "metadata": {},
   "source": [
    "Question 1- What is the most frequent observation (mode) for the column industry?\n",
    "\n",
    "- NA\n",
    "- Tech\n",
    "- healthcare\n",
    "- retail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39036a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent observation (mode) for the column industry is: retail\n"
     ]
    }
   ],
   "source": [
    "# mode of column industry\n",
    "mode_industry = df['industry'].mode()[0]\n",
    "print(f\"The most frequent observation (mode) for the column industry is: {mode_industry}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d80c25",
   "metadata": {},
   "source": [
    "Question 2 - Create the correlation matrix for the numerical features of your dataset. In a correlation matrix, you compute the correlation coefficient between every pair of features.\n",
    "\n",
    "What are the two features that have the biggest correlation?\n",
    "\n",
    "- interaction_count and lead_score\n",
    "- number_of_courses_viewed and lead_score\n",
    "- number_of_courses_viewed and interaction_count\n",
    "- annual_income and interaction_count\n",
    "- Only consider the pairs above when answering this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "901644d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          number_of_courses_viewed  annual_income  \\\n",
      "number_of_courses_viewed                  1.000000       0.009770   \n",
      "annual_income                             0.009770       1.000000   \n",
      "interaction_count                        -0.023565       0.027036   \n",
      "lead_score                               -0.004879       0.015610   \n",
      "converted                                 0.435914       0.053131   \n",
      "\n",
      "                          interaction_count  lead_score  converted  \n",
      "number_of_courses_viewed          -0.023565   -0.004879   0.435914  \n",
      "annual_income                      0.027036    0.015610   0.053131  \n",
      "interaction_count                  1.000000    0.009888   0.374573  \n",
      "lead_score                         0.009888    1.000000   0.193673  \n",
      "converted                          0.374573    0.193673   1.000000  \n"
     ]
    }
   ],
   "source": [
    "# creating correlation matrix for numerical features\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "145ede91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between interaction_count and lead_score: 0.0099\n",
      "Correlation between number_of_courses_viewed and lead_score: -0.0049\n",
      "Correlation between number_of_courses_viewed and interaction_count: -0.0236\n",
      "Correlation between annual_income and interaction_count: 0.0270\n"
     ]
    }
   ],
   "source": [
    "pairs = [\n",
    "    ('interaction_count', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'lead_score'),\n",
    "    ('number_of_courses_viewed', 'interaction_count'),\n",
    "    ('annual_income', 'interaction_count')\n",
    "]\n",
    "\n",
    "for a, b in pairs:\n",
    "    print(f\"Correlation between {a} and {b}: {correlation_matrix.loc[a, b]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab39ee3a",
   "metadata": {},
   "source": [
    "Among the options - annual_income and interaction_count has highest correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba40d01",
   "metadata": {},
   "source": [
    "Split the data\n",
    "- Split your data in train/val/test sets with 60%/20%/20% distribution.\n",
    "- Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
    "- Make sure that the target value y is not in your dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77844ddb",
   "metadata": {},
   "source": [
    "Question 3: Calculate the mutual information score between y and other categorical variables in the dataset. Use the training set only.\n",
    "Round the scores to 2 decimals using round(score, 2).\n",
    "Which of these variables has the biggest mutual information score?\n",
    "\n",
    "- industry\n",
    "- location\n",
    "- lead_source\n",
    "- employment_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940e096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (877, 9), Validation set: (292, 9), Test set: (293, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(columns=['converted'])\n",
    "y = df['converted']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(df, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42,stratify=y_temp)\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a10817bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  MI Score\n",
      "0        lead_source      0.03\n",
      "1           industry      0.02\n",
      "2  employment_status      0.02\n",
      "3           location      0.00\n"
     ]
    }
   ],
   "source": [
    "#Mutual Information\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "mi_scores = mutual_info_classif(X_train[categorical_cols].apply(lambda x:x.astype('category').cat.codes), y_train, discrete_features=True, random_state=42)\n",
    "mi_df=pd.DataFrame({'Feature': categorical_cols, 'MI Score': mi_scores})\n",
    "mi_df['MI Score'] = mi_df['MI Score'].round(2)\n",
    "print(mi_df.sort_values(by='MI Score', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2557014",
   "metadata": {},
   "source": [
    "Question 4\n",
    "- Now let's train a logistic regression.\n",
    "- Remember that we have several categorical variables in the dataset. Include them using one-hot encoding.\n",
    "- Fit the model on the training dataset.\n",
    "- To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "- model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "- Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "\n",
    "What accuracy did you get?\n",
    "\n",
    "- 0.64\n",
    "- 0.74\n",
    "- 0.84\n",
    "- 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b3ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Training logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('clf', model)\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred = pipe.predict(X_val)\n",
    "acc = round(accuracy_score(y_val, y_pred), 2)\n",
    "print(\"Validation Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1756f91",
   "metadata": {},
   "source": [
    "Question 5\n",
    "\n",
    "Let's find the least useful feature using the feature elimination technique.\n",
    "Train a model using the same features and parameters as in Q4 (without rounding).\n",
    "Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "Which of following feature has the smallest difference?\n",
    "\n",
    "- 'industry'\n",
    "- 'employment_status'\n",
    "- 'lead_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a369e93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Feature  Accuracy_Drop\n",
      "3             annual_income      -0.167808\n",
      "6         interaction_count      -0.164384\n",
      "2  number_of_courses_viewed      -0.013699\n",
      "4         employment_status      -0.006849\n",
      "1                  industry      -0.003425\n",
      "0               lead_source       0.000000\n",
      "7                lead_score       0.000000\n",
      "5                  location       0.003425\n",
      "8                 converted       0.126712\n"
     ]
    }
   ],
   "source": [
    "#Feature Elimination using Drop-Column Importance\n",
    "base_acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "acc_diffs = {}\n",
    "\n",
    "for col in X_train.columns:\n",
    "    X_train_red = X_train.drop(columns=[col])\n",
    "    X_val_red = X_val.drop(columns=[col])\n",
    "    \n",
    "    # New pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('prep', ColumnTransformer([\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'),\n",
    "             X_train_red.select_dtypes(include='object').columns)\n",
    "        ], remainder='passthrough')),\n",
    "        ('clf', LogisticRegression(solver='liblinear', C=1.0,\n",
    "                                   max_iter=1000, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    pipe.fit(X_train_red, y_train)\n",
    "    acc = accuracy_score(y_val, pipe.predict(X_val_red))\n",
    "    acc_diffs[col] = base_acc - acc\n",
    "\n",
    "diff_df = pd.DataFrame(acc_diffs.items(), columns=['Feature', 'Accuracy_Drop'])\n",
    "diff_df = diff_df.sort_values('Accuracy_Drop')\n",
    "print(diff_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b192550a",
   "metadata": {},
   "source": [
    "from the options 'lead_score' has 0 effect on removal on accuracy; while other options have a positive effect(acccuracy increase post removal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f98c7",
   "metadata": {},
   "source": [
    "Question 6\n",
    "\n",
    "Now let's train a regularized logistic regression.\n",
    "Let's try the following values of the parameter C: [0.01, 0.1, 1, 10, 100].\n",
    "Train models using all the features as in Q4.\n",
    "Calculate the accuracy on the validation dataset and round it to 3 decimal digits.\n",
    "Which of these C leads to the best accuracy on the validation set?\n",
    "\n",
    "- 0.01\n",
    "- 0.1\n",
    "- 1\n",
    "- 10\n",
    "- 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c233bb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=0.01: accuracy=0.846\n",
      "C=0.1: accuracy=0.836\n",
      "C=1: accuracy=0.832\n",
      "C=10: accuracy=0.832\n",
      "C=100: accuracy=0.832\n"
     ]
    }
   ],
   "source": [
    "Cs = [0.01, 0.1, 1, 10, 100]\n",
    "results = []\n",
    "\n",
    "for c in Cs:\n",
    "    model = LogisticRegression(\n",
    "        solver='liblinear', C=c, max_iter=1000, random_state=42\n",
    "    )\n",
    "    pipe = Pipeline([\n",
    "        ('prep', preprocessor),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    acc = accuracy_score(y_val, pipe.predict(X_val))\n",
    "    results.append((c, round(acc, 3)))\n",
    "\n",
    "for c, acc in results:\n",
    "    print(f\"C={c}: accuracy={acc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f599544",
   "metadata": {},
   "source": [
    "post tuning, best accruacy at c=0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
